{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ae4f29-ce08-4546-9d51-01d3d6b1dbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import s3fs\n",
    "import zarr\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "import xarray as xr\n",
    "from dask.utils import parse_bytes\n",
    "import math\n",
    "import pandas as pd\n",
    "import dask\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833332ac-c23f-4b48-8e91-12ad1b2b9917",
   "metadata": {},
   "source": [
    "# Simple MinIO access with s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ddfe89-cedc-4db4-8d27-a59a3cc71315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!aws configure set aws_access_key_id xxx\n",
    "#!aws configure set aws_secret_access_key yyy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c660357b-4cff-4f07-8f01-7bcd743bea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key = !aws configure get aws_access_key_id\n",
    "access_key = access_key[0]\n",
    "secret_key = !aws configure get aws_secret_access_key\n",
    "secret_key = secret_key[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f938612-2a10-48f0-97ef-f6ce5d14f8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_kwargs={'endpoint_url': 'https://pangeo-eosc-minioapi.vm.fedcloud.eu/'}\n",
    "\n",
    "#s3 = s3fs.S3FileSystem(anon=False, client_kwargs=client_kwargs) # Works only when using s3 in this Notebook, not with distributed.\n",
    "s3 = s3fs.S3FileSystem(key=access_key, secret=secret_key, client_kwargs=client_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b164e0-eef5-4843-b614-2ee98db610cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_prefix = os.environ['JUPYTERHUB_USER']\n",
    "# s3_suffix below is what you need to customise to create your own buckets:\n",
    "s3_suffix = 'test'\n",
    "s3_bucket = s3_prefix + '-' + s3_suffix + '/'\n",
    "s3.ls(s3_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89c4de0-848c-4720-b35f-1d699435e7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.touch(s3_bucket + 'myfile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f5cde6-5495-4353-be92-f848d3b42d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.ls(s3_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb691f9-42ab-455e-912d-8a6c7ecdb87b",
   "metadata": {},
   "source": [
    "# Try to write some Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c679e5-aa11-43ea-8aa5-2551192db872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeseries(\n",
    "    chunk_per_worker=5,\n",
    "    chunk_size=\"128 MB\",\n",
    "    num_nodes=12,\n",
    "    worker_per_node=4,\n",
    "    chunking_scheme=None,\n",
    "    lat=320,\n",
    "    lon=384,\n",
    "    start=\"1980-01-01\",\n",
    "    freq=\"1H\",\n",
    "    nan=False,\n",
    "):\n",
    "    \"\"\" Create synthetic Xarray dataset filled with random\n",
    "    data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk_per_worker : int\n",
    "          number of chunk placed per worker.\n",
    "          see docs.dask.org, best practices, for chunk.\n",
    "          Best chunk size is around 100M but, each worker can\n",
    "          have many chunk, which automate the parallelism in dask.\n",
    "    chunk_size : str\n",
    "          chunk size in bytes, kilo, mega or any factor of bytes\n",
    "    num_nodes : int\n",
    "           number of compute nodes\n",
    "    worker_per_node: int\n",
    "           number of dask workers per node\n",
    "    chunking_scheme : str\n",
    "           Whether to chunk across time dimension ('temporal') or\n",
    "           horizontal dimensions (lat, lon) ('spatial').\n",
    "           If None, automatically determine chunk sizes along all dimensions.\n",
    "    lat : int\n",
    "         number of latitude values\n",
    "    lon : int\n",
    "         number of longitude values\n",
    "    start : datetime (or datetime-like string)\n",
    "        Start of time series\n",
    "    freq : string\n",
    "        String like '2s' or '1H' or '12W' for the time series frequency\n",
    "    nan : bool\n",
    "         Whether to include nan in generated data\n",
    "    Examples\n",
    "    ---------\n",
    "    >>> from benchmarks.datasets import timeseries\n",
    "    >>> ds = timeseries('128MB', 5, chunking_scheme='spatial', lat=500, lon=600)\n",
    "    >>> ds\n",
    "    <xarray.Dataset>\n",
    "    Dimensions:  (lat: 500, lon: 600, time: 267)\n",
    "    Coordinates:\n",
    "    * time     (time) datetime64[ns] 1980-01-01 1980-01-02 ... 1980-09-23\n",
    "    * lon      (lon) float64 -180.0 -179.4 -178.8 -178.2 ... 178.8 179.4 180.0\n",
    "    * lat      (lat) float64 -90.0 -89.64 -89.28 -88.92 ... 88.92 89.28 89.64 90.0\n",
    "    Data variables:\n",
    "        sst      (time, lon, lat) float64 dask.array<shape=(267, 600, 500), .....\n",
    "    Attributes:\n",
    "        history:  created for compute benchmarking\n",
    "    \"\"\"\n",
    "\n",
    "    dt = np.dtype(\"f8\")\n",
    "    itemsize = dt.itemsize\n",
    "    chunk_size = parse_bytes(chunk_size)\n",
    "    total_bytes = chunk_size * num_nodes * worker_per_node * chunk_per_worker\n",
    "    # total_bytes = chunk_size * num_nodes * worker_per_node\n",
    "    size = total_bytes / itemsize\n",
    "    timesteps = math.ceil(size / (lat * lon))\n",
    "    shape = (timesteps, lon, lat)\n",
    "    if chunking_scheme == \"temporal\":\n",
    "        x = math.ceil(chunk_size / (lon * lat * itemsize))\n",
    "        chunks = (x, lon, lat)\n",
    "    elif chunking_scheme == \"spatial\":\n",
    "        x = math.ceil(math.sqrt(chunk_size / (timesteps * itemsize)))\n",
    "        chunks = (timesteps, x, x)\n",
    "    else:\n",
    "        chunks = \"auto\"\n",
    "\n",
    "    lats = xr.DataArray(np.linspace(start=-90, stop=90, num=lat), dims=[\"lat\"])\n",
    "    lons = xr.DataArray(np.linspace(start=-180, stop=180, num=lon), dims=[\"lon\"])\n",
    "    times = xr.DataArray(pd.date_range(start=start, freq=freq, periods=timesteps), dims=[\"time\"])\n",
    "    if chunks == \"auto\":\n",
    "        with dask.config.set({\"array.chunk-size\": chunk_size}):\n",
    "            random_data = randn(shape=shape, chunks=chunks, nan=nan)\n",
    "    else:\n",
    "        random_data = randn(shape=shape, chunks=chunks, nan=nan)\n",
    "    ds = xr.DataArray(\n",
    "        random_data,\n",
    "        dims=[\"time\", \"lon\", \"lat\"],\n",
    "        coords={\"time\": times, \"lon\": lons, \"lat\": lats},\n",
    "        name=\"sst\",\n",
    "        attrs={\"units\": \"baz units\", \"description\": \"a description\"},\n",
    "    ).to_dataset()\n",
    "    ds.attrs = {\"history\": \"created for compute benchmarking\"}\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def randn(shape, chunks=None, nan=False, seed=0):\n",
    "    rng = da.random.RandomState(seed)\n",
    "    x = 5 + 3 * rng.standard_normal(shape, chunks=chunks)\n",
    "    if nan:\n",
    "        x = da.where(x < 0, np.nan, x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2058ae-b011-4686-bc28-a53d1f1e031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = timeseries(chunk_size='16 MiB', chunking_scheme='temporal', chunk_per_worker=1, num_nodes=1)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4608cf-480d-4ac3-96c3-7f997b25a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_s3 = s3fs.S3Map(root=s3_bucket + 'xarray-demo-dask-s3',\n",
    "                   s3=s3,\n",
    "                   check=False)\n",
    "store_s3.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1bf3a7-f95f-4688-8273-898defe36e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds.to_zarr(store=store_s3, mode='w', consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f601a971-585c-4305-8678-474095447a06",
   "metadata": {},
   "source": [
    "# Try some computation with dask-gateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b35228-608c-4fdf-aa14-d11f0883d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_gateway import Gateway\n",
    "gateway = Gateway()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd079e4-71b2-4a0a-a048-2507933bd5e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = gateway.new_cluster(worker_memory=4)\n",
    "cluster.scale(6)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e9c26e-4cf6-482c-80e4-ebf69bd08564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(cluster)\n",
    "client.wait_for_workers(4)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed6b684-41c3-4c52-b3f2-13a98f7fbe55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "sample = 10_000_000_000  # <- this is huge!\n",
    "xxyy = da.random.uniform(-1, 1, size=(2, sample))\n",
    "norm = da.linalg.norm(xxyy, axis=0)\n",
    "summ = da.sum(norm <= 1)\n",
    "insiders = summ.compute()\n",
    "pi = 4 * insiders / sample\n",
    "print(\"pi ~= {}\".format(pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98c3382-e079-41f7-9788-018c0f5d845a",
   "metadata": {},
   "source": [
    "# Try to write with distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdb9b59-c738-4978-8092-647d04559c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_s3 = s3fs.S3Map(root=s3_bucket + 'xarray-demo-dask-s3-distributed',\n",
    "                   s3=s3,\n",
    "                   check=False)\n",
    "store_s3.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66af75d1-afdf-4e78-8793-4afc163fbc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds.to_zarr(store=store_s3, mode='w', consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027759bb-b2f4-497e-b069-a2ba0073d86d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
